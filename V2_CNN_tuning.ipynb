{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d97a9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.data.dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from losses import compute_contrastive_loss_from_feats\n",
    "from utils import * \n",
    "from models import *\n",
    "\n",
    "from dataset import ImageDataset\n",
    "from training_config import doodles, reals, doodle_size, real_size, NUM_CLASSES\n",
    "\n",
    "fix_seed(0) # zero seed by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c870ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = 'exp_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a35667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel, stride, padding=0, bias=True):\n",
    "        super().__init__()        \n",
    "        self.block = nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            inchannels, \n",
    "                            outchannels, \n",
    "                            kernel_size=kernel, \n",
    "                            stride=stride, \n",
    "                            padding=padding, \n",
    "                            bias=bias\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(outchannels),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f561239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    CHANNELS = [64, 128, 192]\n",
    "    POOL = (1, 1)\n",
    "\n",
    "    def __init__(self, in_c, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layer1 = ConvBlock(in_c, self.CHANNELS[0], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer2 = ConvBlock(self.CHANNELS[0], self.CHANNELS[1], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer3 = ConvBlock(self.CHANNELS[1], self.CHANNELS[2], kernel=3, stride=2, padding=1, bias=True)\n",
    "        pool = nn.AdaptiveAvgPool2d(self.POOL)\n",
    "        self.layers = nn.Sequential(layer1, layer2, layer3, pool)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.nn = nn.Sequential(\n",
    "                    nn.Linear(self.POOL[0] * self.POOL[1] * self.CHANNELS[2], 64),\n",
    "                    nn.Linear(64, num_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        feats = self.layers(x)\n",
    "        feats = feats.flatten(1)\n",
    "        x = self.nn(self.dropout(feats))\n",
    "\n",
    "        if return_feats:\n",
    "            return x, feats\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e90e72ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(100, 3, 64, 64)\n",
    "net = ConvNet(3, 9)\n",
    "y = net(x)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b4e3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model1, model2, train_set, val_set, tqdm_on, id, num_epochs, batch_size, learning_rate, c1, c2, t):\n",
    "    # training side\n",
    "    optimizer = torch.optim.AdamW(params=list(model1.parameters()) + list(model2.parameters()),\n",
    "                                  lr=learning_rate, weight_decay=3e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    # load the training data\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=16, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=16,\n",
    "                            pin_memory=True, drop_last=True)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss1_model1 = AverageMeter()\n",
    "        loss1_model2 = AverageMeter()\n",
    "        loss2_model1 = AverageMeter()\n",
    "        loss2_model2 = AverageMeter()\n",
    "        loss3_combined = AverageMeter()\n",
    "        acc_model1 = AverageMeter()\n",
    "        acc_model2 = AverageMeter()\n",
    "\n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        pg = tqdm(train_loader, leave=False, total=len(train_loader), disable=not tqdm_on)\n",
    "        for i, (x1, y1, x2, y2) in enumerate(pg):\n",
    "            # train model1 (doodle)\n",
    "            pred1, feats1 = model1(x1, return_feats=True)\n",
    "            loss_1 = criterion(pred1, y1)    # classification loss\n",
    "            loss_2 = compute_contrastive_loss_from_feats(feats1, y1, t)\n",
    "            loss1_model1.update(loss_1)\n",
    "            loss2_model1.update(loss_2)\n",
    "            loss_model1 = loss_1 + c1 * loss_2\n",
    "\n",
    "            # train model2 (real)\n",
    "            pred2, feats2 = model2(x2, return_feats=True)\n",
    "            loss_1 = criterion(pred2, y2)   # classification loss\n",
    "            loss_2 = compute_contrastive_loss_from_feats(feats2, y2, t)\n",
    "            loss1_model2.update(loss_1)\n",
    "            loss2_model2.update(loss_2)\n",
    "            loss_model2 = loss_1 + c1 * loss_2\n",
    "\n",
    "            # the third loss\n",
    "            combined_feat = feats1 * feats2\n",
    "            loss_3 = compute_contrastive_loss_from_feats(combined_feat, y1, t)\n",
    "            loss3_combined.update(loss_3)\n",
    "\n",
    "            loss = loss_model1 + loss_model2 + c2 * loss_3\n",
    "\n",
    "            # statistics\n",
    "            acc_model1.update(compute_accuracy(pred1, y1))\n",
    "            acc_model2.update(compute_accuracy(pred2, y2))\n",
    "\n",
    "            # optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # display\n",
    "            pg.set_postfix({\n",
    "                'acc 1': '{:.6f}'.format(acc_model1.avg),\n",
    "                'acc 2': '{:.6f}'.format(acc_model2.avg),\n",
    "                'l1m1': '{:.6f}'.format(loss1_model1.avg),\n",
    "                'l2m1': '{:.6f}'.format(loss2_model1.avg),\n",
    "                'l1m2': '{:.6f}'.format(loss1_model2.avg),\n",
    "                'l2m2': '{:.6f}'.format(loss2_model2.avg),\n",
    "                'train epoch': '{:03d}'.format(epoch)\n",
    "            })\n",
    "\n",
    "        print(f'train epoch {epoch}, acc 1={acc_model1.avg:.3f}, acc 2={acc_model2.avg:.3f}, l1m1={loss1_model1.avg:.3f},'\n",
    "              f'l1m2={loss1_model2.avg:.3f}, l2m1={loss2_model1.avg:.3f}, l2m2={loss2_model2.avg:.3f}, '\n",
    "              f'l3={loss3_combined.avg:.3f}')\n",
    "\n",
    "        # validation\n",
    "        model1.eval(), model1.eval()\n",
    "        acc_model1.reset(), acc_model2.reset()\n",
    "        pg = tqdm(val_loader, leave=False, total=len(val_loader), disable=not tqdm_on)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x1, y1, x2, y2) in enumerate(pg):\n",
    "                pred1, feats1 = model1(x1, return_feats=True)\n",
    "                pred2, feats2 = model2(x2, return_feats=True)\n",
    "                acc_model1.update(compute_accuracy(pred1, y1))\n",
    "                acc_model2.update(compute_accuracy(pred2, y2))\n",
    "\n",
    "                # display\n",
    "                pg.set_postfix({\n",
    "                    'acc 1': '{:.6f}'.format(acc_model1.avg),\n",
    "                    'acc 2': '{:.6f}'.format(acc_model2.avg),\n",
    "                    'val epoch': '{:03d}'.format(epoch)\n",
    "                })\n",
    "\n",
    "        print(f'validation epoch {epoch}, acc 1 (doodle) = {acc_model1.avg:.3f}, acc 2 (real) = {acc_model2.avg:.3f}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f'training finished')\n",
    "\n",
    "    # save checkpoint\n",
    "    exp_dir = f'exp_data/{id}'\n",
    "    save_model(exp_dir, f'{id}_model1.pt', model1)\n",
    "    save_model(exp_dir, f'{id}_model2.pt', model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4a781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train = True. Doodle list: ['sketchy_doodle', 'tuberlin', 'google_doodles'], \n",
      " real list: ['sketchy_real', 'google_real', 'cifar']. \n",
      " classes: dict_keys(['airplane', 'car', 'cat', 'dog', 'frog', 'horse', 'truck', 'bird', 'ship']) \n",
      "Doodle data size 7022, real data size 46364, ratio 0.15145371408851696\n",
      "Train = False. Doodle list: ['sketchy_doodle', 'tuberlin', 'google_doodles'], \n",
      " real list: ['sketchy_real', 'google_real', 'cifar']. \n",
      " classes: dict_keys(['airplane', 'car', 'cat', 'dog', 'frog', 'horse', 'truck', 'bird', 'ship']) \n",
      "Doodle data size 1764, real data size 9341, ratio 0.18884487742211756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0, acc 1=0.194, acc 2=0.329, l1m1=2.066,l1m2=1.721, l2m1=2.206, l2m2=2.043, l3=2.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch 0, acc 1 (doodle) = 0.185, acc 2 (real) = 0.323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 1, acc 1=0.278, acc 2=0.453, l1m1=1.839,l1m2=1.431, l2m1=2.068, l2m2=1.836, l3=1.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch 1, acc 1 (doodle) = 0.273, acc 2 (real) = 0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 2, acc 1=0.351, acc 2=0.525, l1m1=1.713,l1m2=1.269, l2m1=2.018, l2m2=1.739, l3=1.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch 2, acc 1 (doodle) = 0.252, acc 2 (real) = 0.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 3, acc 1=0.399, acc 2=0.569, l1m1=1.604,l1m2=1.166, l2m1=1.971, l2m2=1.669, l3=1.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch 3, acc 1 (doodle) = 0.393, acc 2 (real) = 0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 4, acc 1=0.449, acc 2=0.592, l1m1=1.493,l1m2=1.105, l2m1=1.922, l2m2=1.628, l3=1.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch 4, acc 1 (doodle) = 0.284, acc 2 (real) = 0.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 5, acc 1=0.495, acc 2=0.626, l1m1=1.377,l1m2=1.026, l2m1=1.858, l2m2=1.565, l3=1.237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch 5, acc 1 (doodle) = 0.421, acc 2 (real) = 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 89/90 [12:45<00:08,  8.27s/it, acc 1=0.528946, acc 2=0.647231, l1m1=1.288378, l2m1=1.809368, l1m2=0.983193, l2m2=1.537351, train epoch=006]"
     ]
    }
   ],
   "source": [
    "train_set = ImageDataset(doodles, reals, doodle_size, real_size, train=True)\n",
    "val_set = ImageDataset(doodles, reals, doodle_size, real_size, train=False)\n",
    "\n",
    "# tunable hyper params.\n",
    "num_epochs, base_bs, base_lr = 10, 512, 2e-2\n",
    "c1, c2, t = 1, 1, 0.1  # contrastive learning. if you want vanilla (cross-entropy) training, set c1 and c2 to 0.\n",
    "dropout = 0.2\n",
    "\n",
    "# models\n",
    "doodle_model = ConvNet(1, NUM_CLASSES, dropout)\n",
    "real_model = ConvNet(3, NUM_CLASSES, dropout)\n",
    "\n",
    "# just some logistics\n",
    "tqdm_on = True     # progress bar\n",
    "id = 25              # change to the id of each experiment accordingly\n",
    "\n",
    "train_model(doodle_model, real_model, train_set, val_set, tqdm_on, id, num_epochs, base_bs, base_lr, c1, c2, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a196d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
