{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72333208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "import time\n",
    "import utils\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232e64ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 9.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.64.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37589e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data.dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from losses import compute_contrastive_loss_from_feats\n",
    "from utils import *  # bad practice, nvm\n",
    "from models import *\n",
    "\n",
    "from dataset import ImageDataset\n",
    "from training_config import doodles, reals, doodle_size, real_size, NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a27e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = 'exp_data'\n",
    "fix_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2180e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2fed255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model1, model2, train_set, val_set, tqdm_on, id, num_epochs, batch_size, learning_rate, c1, c2, t):\n",
    "    # cuda side setup\n",
    "    model1 = nn.DataParallel(model1).cuda()\n",
    "    model2 = nn.DataParallel(model2).cuda()\n",
    "\n",
    "    # training side\n",
    "    optimizer = torch.optim.AdamW(params=list(model1.parameters()) + list(model2.parameters()),\n",
    "                                  lr=learning_rate, weight_decay=3e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    # load the training data\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=16, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=16,\n",
    "                            pin_memory=True, drop_last=True)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss1_model1 = AverageMeter()\n",
    "        loss1_model2 = AverageMeter()\n",
    "        loss2_model1 = AverageMeter()\n",
    "        loss2_model2 = AverageMeter()\n",
    "        loss3_combined = AverageMeter()\n",
    "        acc_model1 = AverageMeter()\n",
    "        acc_model2 = AverageMeter()\n",
    "\n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        pg = tqdm(train_loader, leave=False, total=len(train_loader), disable=not tqdm_on)\n",
    "        for i, (x1, y1, x2, y2) in enumerate(pg):\n",
    "            # doodle, label, real, label\n",
    "            x1, y1, x2, y2 = x1.cuda(), y1.cuda(), x2.cuda(), y2.cuda()\n",
    "\n",
    "            # train model1 (doodle)\n",
    "            pred1, feats1 = model1(x1, return_feats=True)\n",
    "            loss_1 = criterion(pred1, y1)    # classification loss\n",
    "            loss_2 = compute_contrastive_loss_from_feats(feats1, y1, t)\n",
    "            loss1_model1.update(loss_1)\n",
    "            loss2_model1.update(loss_2)\n",
    "            loss_model1 = loss_1 + c1 * loss_2\n",
    "\n",
    "            # train model2 (real)\n",
    "            pred2, feats2 = model2(x2, return_feats=True)\n",
    "            loss_1 = criterion(pred2, y2)   # classification loss\n",
    "            loss_2 = compute_contrastive_loss_from_feats(feats2, y2, t)\n",
    "            loss1_model2.update(loss_1)\n",
    "            loss2_model2.update(loss_2)\n",
    "            loss_model2 = loss_1 + c1 * loss_2\n",
    "\n",
    "            # the third loss\n",
    "            combined_feat = feats1 * feats2\n",
    "            loss_3 = compute_contrastive_loss_from_feats(combined_feat, y1, t)\n",
    "            loss3_combined.update(loss_3)\n",
    "\n",
    "            loss = loss_model1 + loss_model2 + c2 * loss_3\n",
    "\n",
    "            # statistics\n",
    "            acc_model1.update(compute_accuracy(pred1, y1))\n",
    "            acc_model2.update(compute_accuracy(pred2, y2))\n",
    "\n",
    "            # optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # display\n",
    "            pg.set_postfix({\n",
    "                'acc 1': '{:.6f}'.format(acc_model1.avg),\n",
    "                'acc 2': '{:.6f}'.format(acc_model2.avg),\n",
    "                'l1m1': '{:.6f}'.format(loss1_model1.avg),\n",
    "                'l2m1': '{:.6f}'.format(loss2_model1.avg),\n",
    "                'l1m2': '{:.6f}'.format(loss1_model2.avg),\n",
    "                'l2m2': '{:.6f}'.format(loss2_model2.avg),\n",
    "                'train epoch': '{:03d}'.format(epoch)\n",
    "            })\n",
    "\n",
    "        print(f'train epoch {epoch}, acc 1={acc_model1.avg:.3f}, acc 2={acc_model2.avg:.3f}, l1m1={loss1_model1.avg:.3f},'\n",
    "              f'l1m2={loss1_model2.avg:.3f}, l2m1={loss2_model1.avg:.3f}, l2m2={loss2_model2.avg:.3f}, '\n",
    "              f'l3={loss3_combined.avg:.3f}')\n",
    "\n",
    "        # validation\n",
    "        model1.eval(), model1.eval()\n",
    "        acc_model1.reset(), acc_model2.reset()\n",
    "        pg = tqdm(val_loader, leave=False, total=len(val_loader), disable=not tqdm_on)\n",
    "        with torch.no_grad():\n",
    "            for i, (x1, y1, x2, y2) in enumerate(pg):\n",
    "                pred1, feats1 = model1(x1, return_feats=True)\n",
    "                pred2, feats2 = model2(x2, return_feats=True)\n",
    "                acc_model1.update(compute_accuracy(pred1, y1))\n",
    "                acc_model2.update(compute_accuracy(pred2, y2))\n",
    "\n",
    "                # display\n",
    "                pg.set_postfix({\n",
    "                    'acc 1': '{:.6f}'.format(acc_model1.avg),\n",
    "                    'acc 2': '{:.6f}'.format(acc_model2.avg),\n",
    "                    'val epoch': '{:03d}'.format(epoch)\n",
    "                })\n",
    "\n",
    "        print(f'validation epoch {epoch}, acc 1 (doodle) = {acc_model1.avg:.3f}, acc 2 (real) = {acc_model2.avg:.3f}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f'training finished')\n",
    "\n",
    "    # save checkpoint\n",
    "    exp_dir = f'exp_data/{id}'\n",
    "    save_model(exp_dir, f'{id}_model1.pt', model1)\n",
    "    save_model(exp_dir, f'{id}_model2.pt', model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162e2388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ImageDataset(doodles, reals, doodle_size, real_size, train=True)\n",
    "val_set = ImageDataset(doodles, reals, doodle_size, real_size, train=False)\n",
    "\n",
    "# tunable hyper params.\n",
    "use_cnn = True\n",
    "num_epochs, base_bs, base_lr = 10, 512, 2e-2\n",
    "c1, c2, t = 0, 0, 0.1  # contrastive learning. if you want vanilla (cross-entropy) training, set c1 and c2 to 0.\n",
    "dropout = 0.2\n",
    "\n",
    "# models\n",
    "doodle_model = V2ConvNet(1, NUM_CLASSES, dropout)\n",
    "real_model = V2ConvNet(3, NUM_CLASSES, dropout)\n",
    "\n",
    "# just some logistics\n",
    "tqdm_on = False     # progress bar\n",
    "id = 25             # change to the id of each experiment accordingly\n",
    "\n",
    "train_model(doodle_model, real_model, train_set, val_set, tqdm_on, id, num_epochs, base_bs, base_lr, c1, c2, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
