{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedbf04a",
   "metadata": {},
   "source": [
    "# Tip-of-the-Tongue: Doodle-Image Retrieval Engine\n",
    "\n",
    "__Group 1:__ \n",
    "- Rishabh Anand (A0220603Y)\n",
    "- New Jun Jie (TODO)\n",
    "- Ai Bo (TODO)\n",
    "\n",
    "---\n",
    "\n",
    "__Tip of the tongue__ refers to the situation when we have a vague idea of an object in our memory but simply cannot name it. Most often than not, we feel retrieval of the object's name is imminent. However, we can definitely draw out a doodle of this object when asked to. The objective of this CS4243 project is to investigate the design of learning algorithms for retrieving a collection of real-world images from these manually drawn doodles. \n",
    "\n",
    "As this module is on Computer Vision, our project focuses on the dataset collection and preprocessing, as well as model selection, training, and testing _only_. One can easily package the models into a search engine that takes in a doodle and returns the top-k matching images.\n",
    "\n",
    "As a taster, here are some interesting results:\n",
    "<!-- ADD RESULTS -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f786eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93e78f",
   "metadata": {},
   "source": [
    "## Training and Testing Dataset\n",
    "\n",
    "The dataset consists of an amalgam of over 1 million doodles web-scraped from the following sources:\n",
    "\n",
    "- Google Quick, Draw!\n",
    "- Sketchy\n",
    "\n",
    "It also features real-life images web-scraped from:\n",
    "\n",
    "- Google Images \n",
    "-\n",
    "\n",
    "Our final dataset is an unpaired "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8781c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    DATASET_DIR = {True: 'dataset/dataset_train.npy', False: 'dataset/dataset_test.npy'}\n",
    "    DOODLE_KEY = 'sketchy_doodle'  # 'google_doodles'\n",
    "    REAL_KEY = 'sketchy_real'   # 'google_real'\n",
    "\n",
    "    def __init__(self, doodle_size, real_size, train: bool):\n",
    "        super(ImageDataset, self).__init__()\n",
    "        dataset = np.load(self.DATASET_DIR[train], allow_pickle=True)[()]\n",
    "        doodle, real = dataset[self.DOODLE_KEY], dataset[self.REAL_KEY]\n",
    "        self.doodle_dict = doodle\n",
    "        self.real_dict = real\n",
    "\n",
    "        # sanity check\n",
    "        assert doodle.keys() == real.keys(), f'doodle and real images label classes do not match'\n",
    "\n",
    "        # process classes\n",
    "        label_idx = {}\n",
    "        for key in doodle.keys():\n",
    "            if key not in label_idx:\n",
    "                label_idx[key] = len(label_idx)\n",
    "        self.label_idx = label_idx\n",
    "\n",
    "        # parse data and labels\n",
    "        self.doodle_data, self.doodle_label = self._return_x_y_pairs(doodle, label_idx)\n",
    "        self.real_data, self.real_label = self._return_x_y_pairs(real, label_idx)\n",
    "\n",
    "        # data preprocessing\n",
    "        self.doodle_preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(doodle_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.doodle_data.mean(), self.doodle_data.std())\n",
    "        ])\n",
    "        self.real_preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(real_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.real_data.mean(axis=(0,1,2)), self.real_data.std(axis=(0,1,2)))\n",
    "        ])\n",
    "\n",
    "        print(f'Dataset {self.DOODLE_KEY} and {self.REAL_KEY}. '\n",
    "              f'Doodle data size {len(self.doodle_data)}, real data size {len(self.real_data)}, '\n",
    "              f'ratio {len(self.doodle_data)/len(self.real_data)}')\n",
    "\n",
    "    def _return_x_y_pairs(self, data_dict, category_mapping):\n",
    "        xs, ys = [], []\n",
    "        for key in data_dict.keys():\n",
    "            data = data_dict[key]\n",
    "            labels = [category_mapping[key]] * len(data)\n",
    "            xs.append(data)\n",
    "            ys.extend(labels)\n",
    "        return np.concatenate(xs, axis=0), np.array(ys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # naive sampling scheme - sample with replacement\n",
    "        # sample label first so that doodle and real data belong to the same category\n",
    "        label = random.choice(list(self.label_idx.keys()))\n",
    "        doodle_data = self.doodle_preprocess(random.choice(self.doodle_dict[label]))\n",
    "        real_data = self.real_preprocess(random.choice(self.real_dict[label]))\n",
    "        numer_label = self.label_idx[label]\n",
    "        return doodle_data, numer_label, real_data, numer_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.doodle_data), len(self.real_data)) # could be arbitrary number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d1228",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(64, 32, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222a0e8",
   "metadata": {},
   "source": [
    "## Models and Approaches\n",
    "\n",
    "1. Version 1: Multilayer Perceptron Classification\n",
    "2. Version 2: Convolutional Neural Network Classification\n",
    "3. Version 3: Convolutional Neural Network with Contrastive Loss\n",
    "4. Version 4: Convolutional Neural Network with multiple Contrastive Losses\n",
    "5. Version 5: ConvNeXt<sup>1</sup> with multiple Contrastive Losses\n",
    "\n",
    "---\n",
    "\n",
    "<sup>1</sup> Liu, Z., Mao, H., Wu, C. Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A ConvNet for the 2020s. arXiv preprint arXiv:2201.03545."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f1430",
   "metadata": {},
   "source": [
    "## Version 1: Multilayer Perceptron Classification\n",
    "\n",
    "The final architecture and pipeline look like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fea5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.2):\n",
    "        super(ExampleMLP, self).__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.l2 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.l3 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.l4 = nn.Linear(hid_dim, out_dim)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        x = x.flatten(1) # img to vector\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        feat = x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l4(x)\n",
    "\n",
    "        if return_feats:\n",
    "            return x, feat\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9418365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e679f8",
   "metadata": {},
   "source": [
    "## Version 2: Convolutional Neural Network Classification\n",
    "\n",
    "The final pipeline and architecture look like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a34acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layer1 = nn.Sequential(\n",
    "            conv_block(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            conv_block(64, 3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        )\n",
    "        \n",
    "        layer2 = conv_block(128, 192, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer3 = conv_block(192, 256, kernel_size=3, stride=2, padding=1, bias=True)        \n",
    "        pool = nn.AvgPool2d((2,2))\n",
    "        \n",
    "        self.layers = nn.Sequential(layer1, layer2, layer3, pool)\n",
    "        self.nn = nn.Linear(2 * 2 * 256, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        feats = self.layers(x).flatten(1)\n",
    "        out = self.nn(self.dropout(feats))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37f6d9",
   "metadata": {},
   "source": [
    "## Version 3: Convolutional Neural Network with Contrastive Loss\n",
    "\n",
    "#### Contrastive Loss\n",
    "We follow the Contrastive Loss from SimCLR<sup>2</sup>:\n",
    "\n",
    "$$\n",
    "l_{i, j} = -\\log \\frac{\\text{exp}(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{2N}^{k=1} \\mathbb{1}_{k \\neq i} \\text{ exp}(\\text{sim}(z_i, z_k)/\\tau)}\n",
    "$$\n",
    "\n",
    "The total loss is the arithmetic mean of the losses for all positive pairs in a batch:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2N} \\sum^{N}_{k=1} [l(2k-1, 2k) + l(2k, 2k-1)]\n",
    "$$\n",
    "\n",
    "The architecture and pipeline look like so:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<sup>2</sup> Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layer1 = nn.Sequential(\n",
    "            conv_block(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            conv_block(64, 3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        )\n",
    "        \n",
    "        layer2 = conv_block(128, 192, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer3 = conv_block(192, 256, kernel_size=3, stride=2, padding=1, bias=True)        \n",
    "        pool = nn.AvgPool2d((2,2))\n",
    "        \n",
    "        self.layers = nn.Sequential(layer1, layer2, layer3, pool)\n",
    "        self.nn = nn.Linear(2 * 2 * 256, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        feats = self.layers(x).flatten(1)\n",
    "        x = self.nn(self.dropout(feats))\n",
    "\n",
    "        return x, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b594399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim_matrix(feats):\n",
    "    \"\"\"\n",
    "    Takes in a batch of features of size (bs, feat_len).\n",
    "    \"\"\"\n",
    "    sim_matrix = F.cosine_similarity(feats.unsqueeze(2).expand(-1, -1, feats.size(0)),\n",
    "                                     feats.unsqueeze(2).expand(-1, -1, feats.size(0)).transpose(0, 2),\n",
    "                                     dim=1)\n",
    "\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def compute_target_matrix(labels):\n",
    "    \"\"\"\n",
    "    Takes in a label vector of size (bs)\n",
    "    \"\"\"\n",
    "    label_matrix = labels.unsqueeze(-1).expand((labels.shape[0], labels.shape[0]))\n",
    "    trans_label_matrix = torch.transpose(label_matrix, 0, 1)\n",
    "    target_matrix = (label_matrix == trans_label_matrix).type(torch.float)\n",
    "\n",
    "    return target_matrix\n",
    "\n",
    "\n",
    "def contrastive_loss(pred_sim_matrix, target_matrix, temperature):\n",
    "    return F.kl_div(F.softmax(pred_sim_matrix / temperature).log(), F.softmax(target_matrix / temperature),\n",
    "                    reduction=\"batchmean\", log_target=False)\n",
    "\n",
    "\n",
    "def compute_contrastive_loss_from_feats(feats, labels, temperature):\n",
    "    sim_matrix = compute_sim_matrix(feats)\n",
    "    target_matrix = compute_target_matrix(labels)\n",
    "    loss = contrastive_loss(sim_matrix, target_matrix, temperature)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88965d",
   "metadata": {},
   "source": [
    "## Version 4: Convolutional Neural Network with multiple Contrastive Losses\n",
    "\n",
    "We add two more losses to the Contrastive Loss from Version 3.\n",
    "\n",
    "#### Loss 2\n",
    "\n",
    "#### Loss 3\n",
    "\n",
    "The final architecture and pipeline look like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c49eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11a108ec",
   "metadata": {},
   "source": [
    "## Version 5: ConvNeXt with multiple Contrastive Losses\n",
    "\n",
    "<!-- TODO: Talk about ConvNeXt -->\n",
    "\n",
    "Finally, we train ConvNeXt with the three losses used in Version 4. ConvNeXt \n",
    "\n",
    "The final architecture and pipeline look like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aecb130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdbb2cba",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "While the quality of real-life images returned by the model for a given doodle is subjective, we use classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4b064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3171dcb2",
   "metadata": {},
   "source": [
    "## Results and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0fcd3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1477dbdf",
   "metadata": {},
   "source": [
    "## Analysis and Ablations\n",
    "\n",
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3c0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48fee4d3",
   "metadata": {},
   "source": [
    "### GradCAM on ConvNet and ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fd702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
