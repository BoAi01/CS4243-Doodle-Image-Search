{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedbf04a",
   "metadata": {},
   "source": [
    "# Tip-of-the-Tongue: Doodle-Image Retrieval Engine\n",
    "\n",
    "__Group 1:__ \n",
    "- Rishabh Anand (A0220603Y)\n",
    "- New Jun Jie (TODO)\n",
    "- Ai Bo (TODO)\n",
    "\n",
    "---\n",
    "\n",
    "__Tip of the tongue__ refers to the situation when we have a vague idea of an object in our memory but simply cannot name it. Most often than not, we feel retrieval of the object's name is imminent. However, we can definitely draw out a doodle of this object when asked to. The objective of this CS4243 project is to investigate the design of learning algorithms for retrieving a collection of real-world images from these manually drawn doodles. \n",
    "\n",
    "As this module is on Computer Vision, our project focuses on the dataset collection and preprocessing, as well as model selection, training, and testing _only_. One can easily package the models into a search engine that takes in a doodle and returns the top-k matching images.\n",
    "\n",
    "As a taster, here are some interesting results:\n",
    "<!-- ADD RESULTS -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f786eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93e78f",
   "metadata": {},
   "source": [
    "## Training and Testing Dataset\n",
    "\n",
    "The dataset consists of an amalgam of over 1 million doodles web-scraped from the following sources:\n",
    "\n",
    "- Google Quick, Draw!\n",
    "- Sketchy\n",
    "\n",
    "It also features real-life images web-scraped from:\n",
    "\n",
    "- Google Images \n",
    "-\n",
    "\n",
    "Our final dataset is an unpaired "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8781c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_dataset(datasets, size):\n",
    "    combined_dataset = {}\n",
    "    for name, dataset in datasets.items():\n",
    "        for class_name, class_data in dataset.items():\n",
    "            if class_name not in combined_dataset:\n",
    "                combined_dataset[class_name] = []\n",
    "            # resize data so they can be stacked\n",
    "            resized = []\n",
    "            for data in class_data:\n",
    "                resized.append(cv2.resize(data, (size, size), interpolation=cv2.INTER_AREA))\n",
    "            resized = np.stack(resized, axis=0)\n",
    "            combined_dataset[class_name].append(resized)\n",
    "    for class_name, lst_datasets in combined_dataset.items():\n",
    "        combined_dataset[class_name] = np.concatenate(lst_datasets, axis=0)\n",
    "    return combined_dataset\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    DATASET_DIR = {True: 'dataset/dataset_train.npy', False: 'dataset/dataset_test.npy'}\n",
    "\n",
    "    def __init__(self, doodles_list, real_list, doodle_size, real_size, train: bool):\n",
    "        super(ImageDataset, self).__init__()\n",
    "\n",
    "        dataset = np.load(self.DATASET_DIR[train], allow_pickle=True)[()]\n",
    "\n",
    "        doodle_datasets = {name: data for name, data in dataset.items() if name in doodles_list}\n",
    "        real_datasets = {name: data for name, data in dataset.items() if name in real_list}\n",
    "        self.doodle_dict = combined_dataset(doodle_datasets, doodle_size)\n",
    "        self.real_dict = combined_dataset(real_datasets, real_size)\n",
    "\n",
    "        # sanity check\n",
    "        assert set(self.doodle_dict.keys()) == set(self.real_dict.keys()), \\\n",
    "            f'doodle and real images label classes do not match'\n",
    "\n",
    "        # process classes\n",
    "        label_idx = {}\n",
    "        for key in self.doodle_dict.keys():\n",
    "            if key not in label_idx:\n",
    "                label_idx[key] = len(label_idx)\n",
    "        self.label_idx = label_idx\n",
    "\n",
    "        # parse data and labels\n",
    "        self.doodle_data, self.doodle_label = self._return_x_y_pairs(self.doodle_dict, label_idx)\n",
    "        self.real_data, self.real_label = self._return_x_y_pairs(self.real_dict, label_idx)\n",
    "\n",
    "        # data preprocessing\n",
    "        self.doodle_preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(doodle_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((self.doodle_data/255).mean(), (self.doodle_data/255).std())   # IMPORTANT / 255\n",
    "        ])\n",
    "\n",
    "        self.real_preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(real_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((self.real_data/255).mean(axis=(0, 1, 2)), (self.real_data/255).std(axis=(0, 1, 2)))\n",
    "        ])\n",
    "\n",
    "        print(f'Train = {train}. Doodle list: {doodles_list}, \\n real list: {real_list}. \\n classes: {label_idx.keys()} \\n'\n",
    "              f'Doodle data size {len(self.doodle_data)}, real data size {len(self.real_data)}, '\n",
    "              f'ratio {len(self.doodle_data)/len(self.real_data)}')\n",
    "\n",
    "    def _return_x_y_pairs(self, data_dict, category_mapping):\n",
    "        xs, ys = [], []\n",
    "        for key in data_dict.keys():\n",
    "            data = data_dict[key]\n",
    "            labels = [category_mapping[key]] * len(data)\n",
    "            xs.append(data)\n",
    "            ys.extend(labels)\n",
    "        return np.concatenate(xs, axis=0), np.array(ys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # naive sampling scheme - sample with replacement\n",
    "        # sample label first so that doodle and real data belong to the same category\n",
    "        label = random.choice(list(self.label_idx.keys()))\n",
    "        doodle_data = self.doodle_preprocess(random.choice(self.doodle_dict[label]))\n",
    "        real_data = self.real_preprocess(random.choice(self.real_dict[label]))\n",
    "        numer_label = self.label_idx[label]\n",
    "        return doodle_data, numer_label, real_data, numer_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.doodle_data), len(self.real_data))     # could be arbitrary number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d1228",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(64, 32, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222a0e8",
   "metadata": {},
   "source": [
    "## Models and Approaches\n",
    "\n",
    "1. Version 1: Multilayer Perceptron Classification\n",
    "2. Version 2: Convolutional Neural Network Classification\n",
    "3. Version 3: Convolutional Neural Network with Contrastive Loss\n",
    "4. Version 4: Convolutional Neural Network with multiple Contrastive Losses\n",
    "5. Version 5: ConvNeXt<sup>1</sup> with multiple Contrastive Losses\n",
    "\n",
    "---\n",
    "\n",
    "<sup>1</sup> Liu, Z., Mao, H., Wu, C. Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A ConvNet for the 2020s. arXiv preprint arXiv:2201.03545."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f1430",
   "metadata": {},
   "source": [
    "## Version 1: Multilayer Perceptron Classification\n",
    "\n",
    "The final architecture and pipeline look like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fea5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.2):\n",
    "        super(ExampleMLP, self).__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.l2 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.l3 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.l4 = nn.Linear(hid_dim, out_dim)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        x = x.flatten(1) # img to vector\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        feat = x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l4(x)\n",
    "\n",
    "        if return_feats:\n",
    "            return x, feat\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9418365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e679f8",
   "metadata": {},
   "source": [
    "## Version 2: Convolutional Neural Network Classification\n",
    "\n",
    "The final pipeline and architecture look like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a34acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layer1 = nn.Sequential(\n",
    "            conv_block(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            conv_block(64, 3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        )\n",
    "        \n",
    "        layer2 = conv_block(128, 192, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer3 = conv_block(192, 256, kernel_size=3, stride=2, padding=1, bias=True)        \n",
    "        pool = nn.AvgPool2d((2,2))\n",
    "        \n",
    "        self.layers = nn.Sequential(layer1, layer2, layer3, pool)\n",
    "        self.nn = nn.Linear(2 * 2 * 256, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        feats = self.layers(x).flatten(1)\n",
    "        out = self.nn(self.dropout(feats))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8fbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterCNN(nn.Module):\n",
    "    def __init__(self, in_channels, classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, (3,3))\n",
    "        self.conv2 = nn.Conv2d(32, 32, (3,3))\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3,3))\n",
    "        self.conv4 = nn.Conv2d(64, 64, (3,3))\n",
    "        self.mp = nn.MaxPool2d((2,2))\n",
    "        self.flatten = nn.Flatten(1)\n",
    "\n",
    "        self.l1 = nn.Linear(1600, 512)\n",
    "        self.l2 = nn.Linear(512, 128)\n",
    "        self.l3 = nn.Linear(128, classes)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.mp(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.mp(self.conv4(x)))\n",
    "        print (x.shape)\n",
    "        x = self.flatten(x)\n",
    "        print (x.shape)\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.relu(self.l2(x))\n",
    "        out = torch.softmax(self.l3(x), 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c8b7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 64, 5, 5])\n",
      "torch.Size([100, 1600])\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(100, 3, 32, 32)\n",
    "net = BetterCNN(3, 10)\n",
    "y = net(x)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37f6d9",
   "metadata": {},
   "source": [
    "## Version 3: Convolutional Neural Network with Contrastive Loss\n",
    "\n",
    "#### Contrastive Loss\n",
    "We follow the Contrastive Loss from SimCLR<sup>2</sup>:\n",
    "\n",
    "$$\n",
    "l_{i, j} = -\\log \\frac{\\text{exp}(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{2N}^{k=1} \\mathbb{1}_{k \\neq i} \\text{ exp}(\\text{sim}(z_i, z_k)/\\tau)}\n",
    "$$\n",
    "\n",
    "The total loss is the arithmetic mean of the losses for all positive pairs in a batch:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2N} \\sum^{N}_{k=1} [l(2k-1, 2k) + l(2k, 2k-1)]\n",
    "$$\n",
    "\n",
    "The architecture and pipeline look like so:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<sup>2</sup> Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layer1 = nn.Sequential(\n",
    "            conv_block(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            conv_block(64, 3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        )\n",
    "        \n",
    "        layer2 = conv_block(128, 192, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer3 = conv_block(192, 256, kernel_size=3, stride=2, padding=1, bias=True)        \n",
    "        pool = nn.AvgPool2d((2,2))\n",
    "        \n",
    "        self.layers = nn.Sequential(layer1, layer2, layer3, pool)\n",
    "        self.nn = nn.Linear(2 * 2 * 256, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        feats = self.layers(x).flatten(1)\n",
    "        x = self.nn(self.dropout(feats))\n",
    "\n",
    "        return x, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b594399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim_matrix(feats):\n",
    "    \"\"\"\n",
    "    Takes in a batch of features of size (bs, feat_len).\n",
    "    \"\"\"\n",
    "    sim_matrix = F.cosine_similarity(feats.unsqueeze(2).expand(-1, -1, feats.size(0)),\n",
    "                                     feats.unsqueeze(2).expand(-1, -1, feats.size(0)).transpose(0, 2),\n",
    "                                     dim=1)\n",
    "\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def compute_target_matrix(labels):\n",
    "    \"\"\"\n",
    "    Takes in a label vector of size (bs)\n",
    "    \"\"\"\n",
    "    label_matrix = labels.unsqueeze(-1).expand((labels.shape[0], labels.shape[0]))\n",
    "    trans_label_matrix = torch.transpose(label_matrix, 0, 1)\n",
    "    target_matrix = (label_matrix == trans_label_matrix).type(torch.float)\n",
    "\n",
    "    return target_matrix\n",
    "\n",
    "\n",
    "def contrastive_loss(pred_sim_matrix, target_matrix, temperature):\n",
    "    return F.kl_div(F.softmax(pred_sim_matrix / temperature).log(), F.softmax(target_matrix / temperature),\n",
    "                    reduction=\"batchmean\", log_target=False)\n",
    "\n",
    "\n",
    "def compute_contrastive_loss_from_feats(feats, labels, temperature):\n",
    "    sim_matrix = compute_sim_matrix(feats)\n",
    "    target_matrix = compute_target_matrix(labels)\n",
    "    loss = contrastive_loss(sim_matrix, target_matrix, temperature)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88965d",
   "metadata": {},
   "source": [
    "## Version 4: Convolutional Neural Network with multiple Contrastive Losses\n",
    "\n",
    "We add two more losses to the Contrastive Loss from Version 3.\n",
    "\n",
    "#### Loss 2\n",
    "\n",
    "#### Loss 3\n",
    "\n",
    "The final architecture and pipeline look like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a108ec",
   "metadata": {},
   "source": [
    "## Version 5: ConvNeXt with multiple Contrastive Losses\n",
    "\n",
    "<!-- TODO: Talk about ConvNeXt -->\n",
    "\n",
    "Finally, we train ConvNeXt with the three losses used in Version 4. We can think of ConvNeXt as a \"modernised\" ConvNet in hopes of competing head-on with Transformers. \n",
    "\n",
    "ConvNeXt is an improvement over the standard ConvNet that brings together innovations from the Transformer<sup>3</sup> and ResNet<sup>4</sup>. Here are the list of enhancements we wish to showcase in this CS4243 project:\n",
    "\n",
    "1. Block-based \n",
    "\n",
    "The final architecture and pipeline look like so:\n",
    "\n",
    "---\n",
    "<sup>3</sup> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n",
    "\n",
    "<sup>4</sup> He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(dim, dim, (7,7), padding=3, groups=dim)\n",
    "        self.lin1 = nn.Linear(dim, 4 * dim)\n",
    "        self.lin2 = nn.Linear(4 * dim, dim)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_inp = x\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(0, 2, 3, 1) # NCHW -> NHWC\n",
    "        x = self.ln(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = x.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "        out = x + res_inp\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aecb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb2cba",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "While the quality of real-life images returned by the model for a given doodle is subjective, we use classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4b064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3171dcb2",
   "metadata": {},
   "source": [
    "## Results and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0fcd3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1477dbdf",
   "metadata": {},
   "source": [
    "## Analysis and Ablations\n",
    "\n",
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3c0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48fee4d3",
   "metadata": {},
   "source": [
    "### GradCAM on ConvNet and ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fd702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
