{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9ce36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from utils import *\n",
    "from model_training import DoodleDataset, RealDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d14ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(dim, dim, (7, 7), padding=3, groups=dim)\n",
    "        self.lin1 = nn.Linear(dim, 4 * dim)\n",
    "        self.lin2 = nn.Linear(4 * dim, dim)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_inp = x\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # NCHW -> NHWC\n",
    "        x = self.ln(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = x.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "        out = x + res_inp\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    def __init__(self, in_channels, classes, block_dims=[192, 384, 768]):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, block_dims[0], kernel_size=2, stride=2),\n",
    "            ConvNeXtBlock(block_dims[0]),\n",
    "            nn.Conv2d(block_dims[0], block_dims[1], kernel_size=2, stride=2),\n",
    "            ConvNeXtBlock(block_dims[1]),\n",
    "            nn.Conv2d(block_dims[1], block_dims[2], kernel_size=2, stride=2),\n",
    "            ConvNeXtBlock(block_dims[2]),\n",
    "        )\n",
    "        self.block_dims = block_dims\n",
    "        self.project = nn.Linear(block_dims[-1], classes)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        feats = self.blocks(x)\n",
    "        x = feats.view(-1, self.block_dims[-1], 8*8).mean(2)\n",
    "        out = self.project(x)\n",
    "\n",
    "        if return_feats:\n",
    "            return out, feats\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4faf12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 9])\n"
     ]
    }
   ],
   "source": [
    "net = ConvNeXt(3, 9)\n",
    "x = torch.rand(100, 3, 64, 64)\n",
    "y = net(x)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44deece",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ConvNeXt                                 --                        --\n",
      "├─Sequential: 1-1                        [512, 768, 8, 8]          --\n",
      "│    └─Conv2d: 2-1                       [512, 192, 32, 32]        2,496\n",
      "│    └─ConvNeXtBlock: 2-2                [512, 192, 32, 32]        --\n",
      "│    │    └─Conv2d: 3-1                  [512, 192, 32, 32]        9,600\n",
      "│    │    └─LayerNorm: 3-2               [512, 32, 32, 192]        384\n",
      "│    │    └─Linear: 3-3                  [512, 32, 32, 768]        148,224\n",
      "│    │    └─Linear: 3-4                  [512, 32, 32, 192]        147,648\n",
      "│    │    └─GELU: 3-5                    [512, 32, 32, 192]        --\n",
      "│    └─Conv2d: 2-3                       [512, 384, 16, 16]        295,296\n",
      "│    └─ConvNeXtBlock: 2-4                [512, 384, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-6                  [512, 384, 16, 16]        19,200\n",
      "│    │    └─LayerNorm: 3-7               [512, 16, 16, 384]        768\n",
      "│    │    └─Linear: 3-8                  [512, 16, 16, 1536]       591,360\n",
      "│    │    └─Linear: 3-9                  [512, 16, 16, 384]        590,208\n",
      "│    │    └─GELU: 3-10                   [512, 16, 16, 384]        --\n",
      "│    └─Conv2d: 2-5                       [512, 768, 8, 8]          1,180,416\n",
      "│    └─ConvNeXtBlock: 2-6                [512, 768, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-11                 [512, 768, 8, 8]          38,400\n",
      "│    │    └─LayerNorm: 3-12              [512, 8, 8, 768]          1,536\n",
      "│    │    └─Linear: 3-13                 [512, 8, 8, 3072]         2,362,368\n",
      "│    │    └─Linear: 3-14                 [512, 8, 8, 768]          2,360,064\n",
      "│    │    └─GELU: 3-15                   [512, 8, 8, 768]          --\n",
      "├─Linear: 1-2                            [512, 9]                  6,921\n",
      "==========================================================================================\n",
      "Total params: 7,754,889\n",
      "Trainable params: 7,754,889\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 90.68\n",
      "==========================================================================================\n",
      "Input size (MB): 25.17\n",
      "Forward/backward pass size (MB): 11274.33\n",
      "Params size (MB): 31.02\n",
      "Estimated Total Size (MB): 11330.51\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print (summary(net, input_size=(512, 3, 64, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4acfe35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_set = RealDataset(train=True)\n",
    "real_val_set = RealDataset(train=False)\n",
    "\n",
    "real_train_loader = torch.utils.data.DataLoader(real_train_set, batch_size=512, shuffle=True)\n",
    "real_val_loader = torch.utils.data.DataLoader(real_val_set, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ccb885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11141, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print (real_train_set.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87131fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = ConvNeXt(3, 9)\n",
    "optim = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f8bfb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(0)  # zero seed by default\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "model = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d98a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, label):\n",
    "    pred, label = pred.cpu(), label.cpu()\n",
    "    return (pred.argmax(1) == label).sum().item() / len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da0c492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 1.6636805642734875, Avg. Validation Accuracy: 0.3922643195694087\n",
      "Epoch: 1, Training Loss: 1.5064737038178877, Avg. Validation Accuracy: 0.46785194591318063\n",
      "Epoch: 2, Training Loss: 1.4061995527961038, Avg. Validation Accuracy: 0.5288675891709512\n",
      "Epoch: 3, Training Loss: 1.2786825624379246, Avg. Validation Accuracy: 0.5662485485072447\n",
      "Epoch: 4, Training Loss: 1.1857111074707725, Avg. Validation Accuracy: 0.5928820712345174\n",
      "Epoch: 5, Training Loss: 1.117525420405648, Avg. Validation Accuracy: 0.6036269974000935\n",
      "Epoch: 6, Training Loss: 1.0351213135502555, Avg. Validation Accuracy: 0.6339214313361766\n",
      "Epoch: 7, Training Loss: 0.9688996320421045, Avg. Validation Accuracy: 0.6899125361503856\n",
      "Epoch: 8, Training Loss: 0.8632748425006866, Avg. Validation Accuracy: 0.7318433318970553\n",
      "Epoch: 9, Training Loss: 0.7721823399717157, Avg. Validation Accuracy: 0.7645902408565085\n",
      "Epoch: 10, Training Loss: 0.6602438634092157, Avg. Validation Accuracy: 0.8196209869420426\n",
      "Epoch: 11, Training Loss: 0.565673213113438, Avg. Validation Accuracy: 0.8353080633617668\n",
      "Epoch: 12, Training Loss: 0.47869172421368683, Avg. Validation Accuracy: 0.9005289276115914\n",
      "Epoch: 13, Training Loss: 0.3716625652529977, Avg. Validation Accuracy: 0.9054398114337462\n",
      "Epoch: 14, Training Loss: 0.2808467312292619, Avg. Validation Accuracy: 0.9345134851892966\n",
      "Epoch: 15, Training Loss: 0.24156797812743622, Avg. Validation Accuracy: 0.9335793720057256\n",
      "Epoch: 16, Training Loss: 0.19302464750680057, Avg. Validation Accuracy: 0.9364483524187895\n",
      "Epoch: 17, Training Loss: 0.16085628487847067, Avg. Validation Accuracy: 0.9667407323556906\n",
      "Epoch: 18, Training Loss: 0.1244866746393117, Avg. Validation Accuracy: 0.9674411460767703\n",
      "Epoch: 19, Training Loss: 0.08004304123195735, Avg. Validation Accuracy: 0.9834675888057958\n"
     ]
    }
   ],
   "source": [
    "epochs= 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    count = 0\n",
    "    for idx, (x, y) in enumerate(real_train_loader):\n",
    "        count += 1\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        total_loss += loss.detach().cpu().item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    epoch_loss = total_loss / count\n",
    "    \n",
    "    total_val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for idx, (x, y) in enumerate(real_val_loader):\n",
    "            count += 1\n",
    "            pred = model(x)\n",
    "            val_acc = get_accuracy(pred, y)\n",
    "            total_val_acc += val_acc\n",
    "            \n",
    "        avg_val_acc = total_val_acc / count\n",
    "        \n",
    "    print (\"Epoch: {}, Training Loss: {}, Avg. Validation Accuracy: {}\".format(epoch, epoch_loss, avg_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0adce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(ckpt_dir, cp_name, model):\n",
    "    \"\"\"\n",
    "    Create directory /Checkpoint under exp_data_path and save encoder as cp_name\n",
    "    \"\"\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    saving_model_path = os.path.join(ckpt_dir, cp_name)\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module  # convert to non-parallel form\n",
    "    torch.save(model.state_dict(), saving_model_path)\n",
    "    print(f'Model saved: {saving_model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb53921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: v5_trained_real_imgs_classification/v5_model.pt\n"
     ]
    }
   ],
   "source": [
    "exp_dir = f'v5_trained_real_imgs_classification/'\n",
    "save_model(exp_dir, f'v5_model.pt', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd07fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_model.pt\r\n"
     ]
    }
   ],
   "source": [
    "!cd v5_trained_real_imgs_classification && ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
